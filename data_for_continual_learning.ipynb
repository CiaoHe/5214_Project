{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv  test.csv\t\ttrain_clean.json  train_split.json\r\n",
      "test_clean.csv\t       test_split.json\ttrain.csv\t  v1\r\n",
      "test_clean.json        train_clean.csv\ttrain_split.csv   val_split.csv\r\n"
     ]
    }
   ],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_list = []\n",
    "df = pd.read_csv('./data/{}_clean.csv'.format('train'))\n",
    "\n",
    "\n",
    "topic_split_dict = defaultdict(list)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    \n",
    "    obj = {\n",
    "        \"label\": row['target'],\n",
    "        \"sentence\": row['text']\n",
    "    }\n",
    "\n",
    "    json_list.append(obj)\n",
    "    topic_split_dict[row[\"keyword\"]].append(obj)\n",
    "    \n",
    "# with open('./data/{}_split.json'.format('train'), 'a') as outfile:\n",
    "#     for item in json_list:\n",
    "#         json.dump(item, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(df['keyword'].tolist())\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df[df[\"keyword\"] == 'electrocute']['text']:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_clusters = {\n",
    "    'rain_storm': ['rainstorm','windstorm','thunderstorm','cyclone','twister','tornado','typhoon',\n",
    "                   'violent%20storm','hurricane', 'storm','tsunami'],\n",
    "    # 'thunder','lightning', 'whirlwind', \n",
    "    'earth_quake': ['earthquake', 'seismic', 'epicentre', 'aftershock'],\n",
    "    'flood': ['flood', 'flooding', 'floods', 'inundated', 'inundation'],\n",
    "    'nuclear': ['nuclear%20disaster', 'nuclear%20reactor', 'radiation%20emergency'],\n",
    "    'volcano': ['volcano', 'lava', 'upheaval'],\n",
    "    'snow_storm': ['snowstorm', 'blizzard', 'avalanche'],\n",
    "    'fire': ['buildings%20burning', 'buildings%20on%20fire', 'burned', 'burning', 'burning%20buildings', 'bush%20fires',\n",
    "            'fire', 'fire%20truck', 'flames', 'forest%20fire', 'forest%20fires', 'hellfire', 'wild%20fires',\n",
    "            'wildfire', 'ablaze', 'arson', 'arsonist'],\n",
    "    'hijack': ['hijack', 'hijacker', 'hijacking', 'hostage', 'hostages'],\n",
    "    'bombing': ['bomb', 'bombed', 'bombing', 'suicide%20bomb', 'suicide%20bomber', 'suicide%20bombing', 'exploded', 'explosion'],\n",
    "    'murder': ['mass%20murder', 'mass%20murderer', 'massacre']\n",
    "}\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rain_storm 392\n",
      "earth_quake 114\n",
      "flood 154\n",
      "nuclear 79\n",
      "volcano 99\n",
      "snow_storm 96\n",
      "fire 563\n",
      "hijack 168\n",
      "bombing 272\n",
      "murder 101\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for topic_name, keyword_list in topic_clusters.items():\n",
    "    topic_combined = []\n",
    "    for keyword in keyword_list:\n",
    "        topic_combined.extend([{\"label\": row[1]['target'], \"sentence\": row[1]['text']} \n",
    "                               for row in df[df[\"keyword\"] == keyword].iterrows()])\n",
    "    \n",
    "    print(topic_name, len(topic_combined))\n",
    "    train, test = train_test_split(topic_combined, test_size=0.4, random_state=42)\n",
    "\n",
    "    for phase_label, phase_data in zip(['train', 'test'],[train, test]):\n",
    "        with open('./data/{}.{}.json'.format(topic_name, phase_label), 'a') as outfile:\n",
    "            for item in phase_data:\n",
    "                json.dump(item, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airline.test.json\tepidemic.train.json   README.md\r\n",
      "airline.train.json\texplosion.test.json   tornado.test.json\r\n",
      "bombing.test.json\texplosion.train.json  tornado.train.json\r\n",
      "bombing.train.json\tfire.test.json\t      traffic_accident.test.json\r\n",
      "cyclone.test.json\tfire.train.json       traffic_accident.train.json\r\n",
      "cyclone.train.json\tflood.test.json       volcano.test.json\r\n",
      "earth_quake.test.json\tflood.train.json      volcano.train.json\r\n",
      "earth_quake.train.json\thurricane.test.json\r\n",
      "epidemic.test.json\thurricane.train.json\r\n"
     ]
    }
   ],
   "source": [
    "! ls data/crisis_nlp_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
