{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0e134e05457d34029b6460cd73bbf1ed73f339b5b6d98c95be70b69eba114fe95",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n",
    "import spacy\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_preprocess import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #2. remove unkonwn characrters\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "   \n",
    "    #1. remove http links\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = url.sub(r'',text)\n",
    "    \n",
    "    #3,4. remove #,@ and othet symbols\n",
    "    text = text.replace('#',' ')\n",
    "    text = text.replace('@',' ')\n",
    "    symbols = re.compile(r'[^A-Za-z0-9 ]')\n",
    "    text = symbols.sub(r'',text)\n",
    "    \n",
    "    #5. lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.train_iterator = None\n",
    "        self.val_iterator = None\n",
    "        self.test_iterator = None\n",
    "        self.vocab = {}\n",
    "        self.word_embeddings = {}\n",
    "    \n",
    "    def parse_label(self, label):\n",
    "        return int(label)\n",
    "    \n",
    "    def get_pandas_df(self, PATH, mode='train'):\n",
    "        df = pd.read_csv(PATH)\n",
    "        data_text = df.text.tolist()\n",
    "        if mode in ['train','val']:\n",
    "            labels = df.target.tolist()\n",
    "            data_label = list(map(self.parse_label(), labels))\n",
    "            new_df = pd.DataFrame({\"text\":data_text, \"label\":data_label})\n",
    "        else:\n",
    "            new_df = pd.DataFrame({\"text\":data_text,})\n",
    "        return new_df\n",
    "\n",
    "    def load_data(self, w2v_file, train_file, test_file, val_file=None):\n",
    "        NLP = spacy.load('en')\n",
    "        tokenizer = lambda sent: [x.text for x in NLP.tokenizer(clean_text(sent))]\n",
    "\n",
    "        TEXT = data.Field(sequential = True, tokenize=tokenizer, lower=True, fix_length=self.config.max_sen_len)\n",
    "        LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "        train_datafields = [('text',TEXT),('label',LABEL)]\n",
    "        test_datafields = [('text', TEXT)]\n",
    "\n",
    "        #load train and testt data into torchtext.data.Dataset\n",
    "        train_df = self.get_pandas_df(train_file, mode='train')\n",
    "        train_examples = [data.Example.fromlist(i, train_datafields) for i in train_df.values.tolist()]\n",
    "        train_data = data.Dataset(train_examples, train_datafields)\n",
    "\n",
    "        test_df = self.get_pandas_df(test_file, mode='test')\n",
    "        test_examples = [data.Example.fromlist(i, test_datafields) for i in test_df.values.tolist()]\n",
    "        test_data = data.Dataset(test_examples, test_datafields)\n",
    "\n",
    "        #if validation file exists, then load in the train way; otherwise spilt train_data\n",
    "        if not val_file:\n",
    "            val_df = self.get_pandas_df(val_file, mode='val')\n",
    "            val_examples = [data.Example.fromlist(i, train_datafields) for i in val_df.values.tolist()]\n",
    "            val_data = data.Dataset(val_examples, train_datafields)\n",
    "        else:\n",
    "            train_data, val_data = train_data.split(split_ratio=0.8)\n",
    "        \n",
    "        TEXT.build_vocab(train_data, vectors = Vectors(w2v_file))\n",
    "        self.word_embeddings = TEXT.vocab.vectors\n",
    "        self.vocab = TEXT.vocab \n",
    "\n",
    "        #get train/val[if]/test dataiterator\n",
    "        self.train_iterator = data.BucketIterator(\n",
    "            train_data,\n",
    "            batch_size = self.config.batch_size,\n",
    "            sort_key = lambda x: len(x.text),\n",
    "            repeat = False,\n",
    "            shuffle = True,\n",
    "        )\n",
    "        self.val_iterator, self.test_iterator = data.BucketIterator.splits(\n",
    "            (val_data, test_data),\n",
    "            batch_size = self.config.batch_size,\n",
    "            sort_key = lambda x: len(x.text),\n",
    "            repeat = False,\n",
    "            shuffle = False,\n",
    "        )\n",
    "        print(\"Loaded {} training examples\".format(len(train_data)))\n",
    "        print(\"Loaded {} val examples\".format(len(val_data)))\n",
    "        print(\"Loaded {} test examples\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator):\n",
    "    total_preds = []\n",
    "    total = []\n",
    "    for idx,batch in enumerate(iterator):\n",
    "        if torch.cuda.is_available():\n",
    "            x = batch.text.cuda()\n",
    "        else:\n",
    "            x = batch.text\n",
    "        output = model(x)\n",
    "        y_pred = output.cpu().data.max(1)[1].tolist()\n",
    "        total_preds += y_pred \n",
    "        total += batch.label.cpu().data.tolist()\n",
    "    score = accuracy_score(total, total_preds)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "train_text=train_df.text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "train_text = list(map(clean_text,train_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = lambda sent : list(sent[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = list(map(tokenizer, train_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "max(map(len,train_text))\n",
    "min(map(len,train_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "144 5\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('./data/test.csv')\n",
    "test_text=test_df.text.tolist()\n",
    "test_text = list(map(clean_text,test_text))\n",
    "test_text = list(map(tokenizer, test_text))\n",
    "print(max(map(len,test_text)),min(map(len,test_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}